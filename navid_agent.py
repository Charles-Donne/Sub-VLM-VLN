"""
NaVid æ™ºèƒ½ä½“æ¨¡å—
å®ç°åŸºäºè§†é¢‘-è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€å¯¼èˆªæ™ºèƒ½ä½“
æ”¯æŒå¢é‡å¼è§†è§‰tokenå¤ç”¨ï¼Œæé«˜è¯„ä¼°æ•ˆç‡
"""
import os
import re
import json
import random
from datetime import datetime
from typing import Dict, List, Any, Optional

import torch
import cv2
import numpy as np
import imageio
from tqdm import trange
from habitat import Env
from habitat.core.agent import Agent
from habitat.utils.visualizations import maps

from navid.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from navid.conversation import conv_templates, SeparatorStyle
from navid.model.builder import load_pretrained_model
from navid.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria

# å¯¼å…¥æŒ‡ä»¤åˆ†è§£å™¨
from llm_api.instruction_decomposer import InstructionDecomposer


def evaluate_agent(config, split_id, dataset, model_path, result_path) -> None:
    """
    è¯„ä¼°NaVidæ™ºèƒ½ä½“çš„å¯¼èˆªæ€§èƒ½
    
    Args:
        config: å®éªŒé…ç½®å¯¹è±¡
        split_id: æ•°æ®åˆ†å—ID
        dataset: è¯„ä¼°æ•°æ®é›†
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        result_path: ç»“æœä¿å­˜è·¯å¾„
    """
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(os.path.join(result_path, "log"), exist_ok=True)
    os.makedirs(os.path.join(result_path, "video"), exist_ok=True)
    
    # ç®¡ç†å·²è¯„ä¼°episodeè®°å½•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è¯„ï¼‰
    eval_record = os.path.join(result_path, "evaluated.txt")
    evaluated_ids = set()
    if os.path.exists(eval_record):
        with open(eval_record, "r") as f:
            evaluated_ids = {line.strip() for line in f.readlines()}
    
    print(f"å·²è¯„ä¼°IDæ•°é‡: {len(evaluated_ids)}")
    
    # è¿‡æ»¤æœªè¯„ä¼°çš„episode
    unevaluated = [
        ep for ep in dataset.episodes 
        if str(ep.episode_id) not in evaluated_ids
    ]
    
    # é™åˆ¶å•æ¬¡è¯„ä¼°æ•°é‡
    max_episodes = min(20, len(unevaluated))
    
    if not unevaluated:
        print("æ‰€æœ‰episodeå·²å®Œæˆè¯„ä¼°ï¼")
        return
    
    dataset.episodes = unevaluated[:max_episodes]
    evaluating_ids = [str(ep.episode_id) for ep in dataset.episodes]
    print(f"å³å°†è¯„ä¼°çš„ID: {evaluating_ids}")
    
    # åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = Env(config.TASK_CONFIG, dataset)
    agent = NaVid_Agent(model_path, result_path)
    
    num_episodes = len(env.episodes)
    print(f"å®é™…è¯„ä¼° {num_episodes} ä¸ªepisode")
    
    # æ—©åœå‚æ•°
    EARLY_STOP_ROTATION = config.EVAL.EARLY_STOP_ROTATION
    EARLY_STOP_STEPS = config.EVAL.EARLY_STOP_STEPS
    
    # è¯„ä¼°æŒ‡æ ‡
    # distance_to_goal: åœæ­¢æ—¶æ™ºèƒ½ä½“ä¸ç›®æ ‡ç‚¹çš„è·ç¦»(ç±³)ï¼Œè¶Šå°è¶Šå¥½
    # success: æˆåŠŸç‡ï¼Œæ™ºèƒ½ä½“æ˜¯å¦åœ¨3ç±³å†…åœæ­¢(0æˆ–1)
    # spl: Success weighted by Path Lengthï¼ŒæˆåŠŸç‡ä¸è·¯å¾„æ•ˆç‡çš„ç»¼åˆæŒ‡æ ‡
    #      è®¡ç®—å…¬å¼: success * (æœ€çŸ­è·¯å¾„é•¿åº¦ / å®é™…è·¯å¾„é•¿åº¦)
    #      èŒƒå›´[0,1]ï¼Œè¶Šé«˜è¡¨ç¤ºæ—¢æˆåŠŸåˆé«˜æ•ˆ
    # path_length: æ™ºèƒ½ä½“å®é™…è¡Œèµ°çš„è·¯å¾„é•¿åº¦(ç±³)
    # oracle_success: é¢„è¨€æˆåŠŸç‡ï¼Œæ•´ä¸ªè½¨è¿¹ä¸­æ˜¯å¦æ›¾ç»åˆ°è¾¾è¿‡ç›®æ ‡3ç±³å†…(0æˆ–1)
    #                 ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“æ˜¯å¦æ‰¾åˆ°è¿‡ç›®æ ‡ä½†é”™è¿‡äº†åœæ­¢
    target_key = {"distance_to_goal", "success", "spl", "path_length", "oracle_success"}

    # ç”¨äºç»Ÿè®¡æ‰€æœ‰episodeçš„ç»“æœ
    all_results = []
    
    count = 0
    progress = trange(num_episodes, desc=config.EVAL.IDENTIFICATION+"-{}".format(split_id))
    
    # ä¸»è¯„ä¼°å¾ªç¯
    for _ in progress:
        if count >= max_episodes:
            print(f"å·²è¾¾åˆ°æœ€å¤§episodeæ•° ({max_episodes})ï¼Œæå‰ç»“æŸè¯„ä¼°")
            break
            
        # é‡ç½®ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        env.reset()
        agent.reset()
        
        # æ‰§è¡Œä¸€è½®å®Œæ•´çš„episodeè¯„ä¼°ï¼ˆå°è£…åœ¨agentç±»ä¸­ï¼‰
        iter_step = agent.run_episode(env, EARLY_STOP_ROTATION, EARLY_STOP_STEPS)
            
        # æ”¶é›†æœ¬æ¬¡episodeçš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚è·ç¦»ç›®æ ‡ã€æˆåŠŸç‡ç­‰ï¼‰
        info = env.get_metrics()
        result_dict = dict()
        # åªä¿ç•™å…³å¿ƒçš„è¯„ä¼°æŒ‡æ ‡
        result_dict = {k: info[k] for k in target_key if k in info}
        # è®°å½•å½“å‰episodeçš„å”¯ä¸€ID
        result_dict["id"] = env.current_episode.episode_id
        count += 1

        # ä¿å­˜æœ¬æ¬¡ episode çš„è¯„ä¼°æŒ‡æ ‡åˆ° log ç›®å½•
        log_dir = os.path.join(result_path, "log")
        os.makedirs(log_dir, exist_ok=True)
        stats_path = os.path.join(log_dir, f"stats_{env.current_episode.episode_id}.json")
        with open(stats_path, "w") as f:
            json.dump(result_dict, f, indent=4)
        
        # å°†å½“å‰ç»“æœæ·»åŠ åˆ°æ±‡æ€»åˆ—è¡¨
        all_results.append(result_dict)

        # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡æè¿°ï¼Œæ˜¾ç¤ºå½“å‰è¿›åº¦
        progress.set_description(f"{config.EVAL.IDENTIFICATION}-{split_id} [ep {count}/{max_episodes}]")
    
    # ä¿å­˜å·²è¯„ä¼°episode IDï¼ˆé¿å…é‡å¤è¯„ä¼°ï¼‰
    new_evaluated_ids = {str(ep.episode_id) for ep in dataset.episodes}
    with open(eval_record, "a") as f:
        for ep_id in new_evaluated_ids:
            if ep_id not in evaluated_ids:
                f.write(ep_id + "\n")
                evaluated_ids.add(ep_id)
    
    # è®¡ç®—å¹¶ä¿å­˜æœ¬æ¬¡è¯„ä¼°çš„æ±‡æ€»ç»Ÿè®¡
    if all_results:
        # è½®æ¬¡ç¼–å·ï¼šé€šè¿‡ index æ–‡ä»¶ç¡®ä¿æ¯æ¬¡é€’å¢ä¸”ä¸ä¼šè¦†ç›–
        index_file = os.path.join(result_path, "summary_index.txt")
        try:
            last_idx = int(open(index_file, "r").read().strip()) if os.path.exists(index_file) else 0
        except Exception:
            last_idx = 0
        run_idx = last_idx + 1
        with open(index_file, "w") as idx_f:
            idx_f.write(str(run_idx))

        # æ±‡æ€»æ–‡ä»¶ï¼šç»Ÿä¸€å†™å…¥ä¸€ä¸ª summary.txtï¼ŒæŒ‰è½®æ¬¡è¿½åŠ 
        summary_file = os.path.join(result_path, "summary.txt")
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(summary_file, "a") as f:
            f.write("\n" + "#"*80 + "\n")
            f.write(f"NaVid è¯„ä¼°æ±‡æ€»æŠ¥å‘Š | ç¬¬ {run_idx} æ¬¡è¯„ä¼° | Split {split_id} | {now_str}\n")
            f.write("#"*80 + "\n\n")
            f.write(f"è¯„ä¼°æ ‡è¯†: {getattr(getattr(config, 'EVAL', object()), 'IDENTIFICATION', 'N/A')}\n")
            f.write(f"æœ¬è½®è¯„ä¼°episodeæ•°: {count}\n")
            f.write("\n")

            # åˆ—å‡ºæ‰€æœ‰æµ‹è¯•çš„episode
            f.write("æµ‹è¯•çš„Episodeåˆ—è¡¨:\n")
            for i, result in enumerate(all_results, 1):
                f.write(f"  {i}. Episode {result['id']}\n")
            f.write("\n")

            # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡å€¼
            f.write("è¯„ä¼°æŒ‡æ ‡æ±‡æ€»:\n")
            f.write("-"*40 + "\n")
            metrics_to_average = ["distance_to_goal", "success", "spl", "path_length", "oracle_success"]
            for metric in metrics_to_average:
                values = [r[metric] for r in all_results if metric in r]
                if values:
                    avg_value = sum(values) / len(values)
                    f.write(f"{metric:20s}: {avg_value:.4f}\n")
            f.write("\n")

        print(f"æ–°å¢è¯„ä¼° {len(new_evaluated_ids)} ä¸ªepisode")
        print(f"æ±‡æ€»æŠ¥å‘Šå·²è¿½åŠ åˆ°: {summary_file} (ç¬¬ {run_idx} æ¬¡è¯„ä¼°)")



class NaVid_Agent(Agent):
    """
    NaVidæ™ºèƒ½ä½“ç±»
    åŸºäºè§†é¢‘-è¯­è¨€æ¨¡å‹çš„å¯¼èˆªæ™ºèƒ½ä½“ï¼Œæ”¯æŒï¼š
    - å†å²è§†è§‰tokenå¤ç”¨ï¼ˆæé«˜æ¨ç†æ•ˆç‡ï¼‰
    - å¢é‡å¼å›¾åƒå¤„ç†
    - åŠ¨ä½œåºåˆ—ç¼“å­˜
    - åˆ†å±‚æŒ‡ä»¤æ‰§è¡Œï¼ˆinstruction decompositionï¼‰
    """
    
    def __init__(self, model_path, result_path, require_map=True):
        """
        åˆå§‹åŒ–NaVidæ™ºèƒ½ä½“
        
        Args:
            model_path: æ¨¡å‹æƒé‡è·¯å¾„
            result_path: ç»“æœä¿å­˜è·¯å¾„
            require_map: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–åœ°å›¾è§†é¢‘
        """
        print("Initialize NaVid")
        
        self.result_path = result_path
        self.require_map = require_map
        self.conv_mode = "vicuna_v1"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.result_path, exist_ok=True)
        os.makedirs(os.path.join(self.result_path, "log"), exist_ok=True)
        os.makedirs(os.path.join(self.result_path, "video"), exist_ok=True)

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model_name = get_model_name_from_path(model_path)
        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
            model_path, None, get_model_name_from_path(model_path)
        )

        print("Initialization Complete")

        
        self.promt_template = "Imagine you are a robot programmed for navigation tasks. You have been given a video of historical observations and an image of the current observation <image>. Your assigned task is: '{}'. Analyze this series of images to decide your next move, which could involve turning left or right by a specific degree or moving forward a certain distance."

        self.history_rgb_tensor = None
        
        self.rgb_list = []
        self.topdown_map_list = []

        self.count_id = 0
        
        # ğŸ†• åˆå§‹åŒ–æŒ‡ä»¤åˆ†è§£å™¨
        self.decomposer = InstructionDecomposer()
        
        self.reset()
    
    def run_episode(self, env, early_stop_rotation=20, early_stop_steps=500):
        """
        æ‰§è¡Œä¸€è½®å®Œæ•´çš„episodeè¯„ä¼°å¾ªç¯ï¼ˆæ”¯æŒåˆ†å±‚æŒ‡ä»¤æ‰§è¡Œï¼‰
        
        æµç¨‹:
        1. è·å–åŸå§‹æŒ‡ä»¤å¹¶åˆ†è§£ä¸ºå­æŒ‡ä»¤åºåˆ—
        2. å¤–å±‚å¾ªç¯ï¼šéå†æ¯ä¸ªå­æŒ‡ä»¤
        3. æ¯ä¸ªå­æŒ‡ä»¤å¼€å§‹å‰ï¼šé‡ç½®è§†è§‰å†å²ï¼ˆrgb_list, history_rgb_tensorï¼‰
        4. å†…å±‚å¾ªç¯ï¼šæ‰§è¡Œå½“å‰å­æŒ‡ä»¤ç›´åˆ°æ¨¡å‹è¾“å‡ºstop
        5. ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå­æŒ‡ä»¤
        
        Args:
            env: Habitatç¯å¢ƒå¯¹è±¡
            early_stop_rotation: æœ€å¤§è¿ç»­æ—‹è½¬æ¬¡æ•°ï¼ˆæ—©åœé˜ˆå€¼ï¼‰
            early_stop_steps: æœ€å¤§æ­¥æ•°ï¼ˆæ—©åœé˜ˆå€¼ï¼‰
            
        Returns:
            obs: æœ€åä¸€æ­¥çš„è§‚æµ‹
            iter_step: æ€»æ­¥æ•°
        """
        # åˆå§‹åŒ–ç¯å¢ƒå¹¶è·å–åŸå§‹æŒ‡ä»¤
        obs = env.reset()
        original_instruction = obs["instruction"]["text"]
        
        # ã€æ­¥éª¤1ã€‘åˆ†è§£æŒ‡ä»¤ä¸ºå­æŒ‡ä»¤åºåˆ—
        print(f"\n{'='*80}")
        print(f"ğŸ¯ åŸå§‹æŒ‡ä»¤: {original_instruction}")
        print(f"{'='*80}")
        sub_instructions = self.decomposer.decompose(original_instruction)
        
        # æ€»æ­¥æ•°è®¡æ•°å™¨
        total_iter_step = 0
        continuse_rotation_count = 0
        last_dtg = 999
        
        # ã€æ­¥éª¤2ã€‘å¤–å±‚å¾ªç¯ï¼šéå†æ¯ä¸ªå­æŒ‡ä»¤
        for sub_idx, sub_inst_dict in enumerate(sub_instructions, 1):
            # æå–å­æŒ‡ä»¤æ–‡æœ¬ï¼ˆInstructionDecomposer ä¿è¯åŒ…å« 'sub_instruction' é”®ï¼‰
            sub_instruction = sub_inst_dict['sub_instruction']
            
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“ å­ä»»åŠ¡ [{sub_idx}/{len(sub_instructions)}]: {sub_instruction}")
            print(f"{'â”€'*80}")
            
            # ã€æ­¥éª¤3ã€‘é‡ç½®è§†è§‰å†å²ï¼ˆæ¯ä¸ªå­ä»»åŠ¡ç‹¬ç«‹ï¼‰
            self.rgb_list = []
            self.history_rgb_tensor = None
            
            # ã€æ­¥éª¤4ã€‘å†…å±‚å¾ªç¯ï¼šæ‰§è¡Œå½“å‰å­æŒ‡ä»¤ç›´åˆ°stop
            sub_iter_step = 0
            
            while True:
                # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸï¼ˆæ•´ä¸ªä»»åŠ¡ç»ˆæ­¢æ¡ä»¶ï¼‰
                if env.episode_over:
                    print(f"\nğŸ Episode ç»“æŸï¼ˆå®Œæˆ {sub_idx}/{len(sub_instructions)} ä¸ªå­ä»»åŠ¡ï¼Œæ€»æ­¥æ•° {total_iter_step}ï¼‰")
                    return total_iter_step
                
                info = env.get_metrics()
                
                # æ£€æµ‹æ˜¯å¦æŒç»­åŸåœ°æ—‹è½¬
                if info["distance_to_goal"] != last_dtg:
                    last_dtg = info["distance_to_goal"]
                    continuse_rotation_count = 0
                else:
                    continuse_rotation_count += 1
                
                # è·å–æ™ºèƒ½ä½“åŠ¨ä½œï¼ˆä¼ é€’å­æŒ‡ä»¤æ–‡æœ¬ï¼‰
                action = self.act(obs, info, env.current_episode.episode_id, sub_instruction=sub_instruction)
                
                # æ—©åœæ¡ä»¶ï¼šè¿‡å¤šæ—‹è½¬æˆ–è¶…è¿‡æœ€å¤§æ­¥æ•°
                if continuse_rotation_count > early_stop_rotation or total_iter_step > early_stop_steps:
                    print(f"âš ï¸  è§¦å‘æ—©åœæ¡ä»¶ï¼ˆæ—‹è½¬:{continuse_rotation_count}, æ€»æ­¥æ•°:{total_iter_step}ï¼‰")
                    action = {"action": 0}  # å¼ºåˆ¶åœæ­¢
                    env.step(action)
                    return total_iter_step
                
                sub_iter_step += 1
                total_iter_step += 1
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºstopåŠ¨ä½œï¼ˆå­ä»»åŠ¡å®Œæˆï¼Œé™é»˜è¿›å…¥ä¸‹ä¸€ä¸ªå­ä»»åŠ¡ï¼‰
                if action["action"] == 0:

                    break  # é€€å‡ºå†…å±‚å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå­æŒ‡ä»¤
                # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–æ–°è§‚æµ‹

                obs = env.step(action)
                

        obs = env.step(action)

        return total_iter_step


    def process_images(self, rgb_list):
        """
        å¢é‡å¼å›¾åƒå¤„ç†ï¼šåªå¤„ç†æ–°å¢å›¾åƒï¼Œå¤ç”¨å†å²è§†è§‰token
        æ˜¾è‘—æå‡å¤šæ­¥å¯¼èˆªä»»åŠ¡çš„æ¨ç†æ•ˆç‡
        
        Args:
            rgb_list: RGBå›¾åƒåˆ—è¡¨
            
        Returns:
            åŒ…å«å®Œæ•´è§†è§‰tokençš„åˆ—è¡¨
        """
        # ç¡®å®šéœ€è¦å¤„ç†çš„èµ·å§‹ç´¢å¼•
        start_img_index = 0
        if self.history_rgb_tensor is not None:
            start_img_index = self.history_rgb_tensor.shape[0]
        
        # åªå¤„ç†æ–°å¢å›¾åƒ
        batch_image = np.asarray(rgb_list[start_img_index:])
        video = self.image_processor.preprocess(batch_image, return_tensors='pt')['pixel_values'].half().cuda()

        # æ‹¼æ¥å†å²å’Œæ–°å¢token
        if self.history_rgb_tensor is None:
            self.history_rgb_tensor = video
        else:
            self.history_rgb_tensor = torch.cat((self.history_rgb_tensor, video), dim=0)
        
        return [self.history_rgb_tensor]



    def predict_inference(self, prompt):
        """
        æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼Œç”Ÿæˆå¯¼èˆªå†³ç­–
        
        Args:
            prompt: åŒ…å«ä»»åŠ¡æŒ‡ä»¤çš„æç¤ºè¯
            
        Returns:
            æ¨¡å‹è¾“å‡ºçš„å¯¼èˆªæŒ‡ä»¤ï¼ˆå¦‚"turn left 30 degrees"ï¼‰
        """
        question = prompt.replace(DEFAULT_IMAGE_TOKEN, '').replace('\n', '')
        qs = prompt

        # å®šä¹‰ç‰¹æ®Štoken
        VIDEO_START_SPECIAL_TOKEN = "<video_special>"
        VIDEO_END_SPECIAL_TOKEN = "</video_special>"
        IMAGE_START_TOKEN = "<image_special>"
        IMAGE_END_TOKEN = "</image_special>"
        NAVIGATION_SPECIAL_TOKEN = "[Navigation]"
        IAMGE_SEPARATOR = "<image_sep>"
        
        # tokenåŒ–ç‰¹æ®Šæ ‡è®°
        image_start_special_token = self.tokenizer(IMAGE_START_TOKEN, return_tensors="pt").input_ids[0][1:].cuda()
        image_end_special_token = self.tokenizer(IMAGE_END_TOKEN, return_tensors="pt").input_ids[0][1:].cuda()
        video_start_special_token = self.tokenizer(VIDEO_START_SPECIAL_TOKEN, return_tensors="pt").input_ids[0][1:].cuda()
        video_end_special_token = self.tokenizer(VIDEO_END_SPECIAL_TOKEN, return_tensors="pt").input_ids[0][1:].cuda()
        navigation_special_token = self.tokenizer(NAVIGATION_SPECIAL_TOKEN, return_tensors="pt").input_ids[0][1:].cuda()
        image_seperator = self.tokenizer(IAMGE_SEPARATOR, return_tensors="pt").input_ids[0][1:].cuda()

        # æ„å»ºæç¤ºè¯
        if self.model.config.mm_use_im_start_end:
            qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs.replace('<image>', '')
        else:
            qs = DEFAULT_IMAGE_TOKEN + '\n' + qs.replace('<image>', '')

        # ä½¿ç”¨å¯¹è¯æ¨¡æ¿
        conv = conv_templates[self.conv_mode].copy()
        conv.append_message(conv.roles[0], qs)
        conv.append_message(conv.roles[1], None)
        prompt = conv.get_prompt()

        # tokenåŒ–å¹¶æ’å…¥ç‰¹æ®Šæ ‡è®°
        token_prompt = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').cuda()
        indices_to_replace = torch.where(token_prompt == -200)[0]
        new_list = []
        
        while indices_to_replace.numel() > 0:
            idx = indices_to_replace[0]
            new_list.append(token_prompt[:idx])
            new_list.append(video_start_special_token)
            new_list.append(image_seperator)
            new_list.append(token_prompt[idx:idx + 1])
            new_list.append(video_end_special_token)
            new_list.append(image_start_special_token)
            new_list.append(image_end_special_token)
            new_list.append(navigation_special_token)
            token_prompt = token_prompt[idx + 1:]
            indices_to_replace = torch.where(token_prompt == -200)[0]
            
        if token_prompt.numel() > 0:
            new_list.append(token_prompt)
        input_ids = torch.cat(new_list, dim=0).unsqueeze(0)

        # åœæ­¢æ¡ä»¶
        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
        keywords = [stop_str]
        stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)

        # å¤„ç†å›¾åƒ
        imgs = self.process_images(self.rgb_list)

        # æ¨¡å‹ç”Ÿæˆ
        cur_prompt = question
        with torch.inference_mode():
            self.model.update_prompt([[cur_prompt]])
            output_ids = self.model.generate(
                input_ids,
                images=imgs,
                do_sample=True,
                temperature=0.2,
                max_new_tokens=1024,
                use_cache=True,
                stopping_criteria=[stopping_criteria]
            )

        # è§£ç è¾“å‡º
        input_token_len = input_ids.shape[1]
        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
        if n_diff_input_output > 0:
            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
        
        outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
        outputs = outputs.strip()
        if outputs.endswith(stop_str):
            outputs = outputs[:-len(stop_str)]
        outputs = outputs.strip()

        return outputs




    def extract_result(self, output):
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–åŠ¨ä½œå’Œå‚æ•°
        
        Args:
            output: æ¨¡å‹è¾“å‡ºæ–‡æœ¬
            
        Returns:
            (action_id, value): åŠ¨ä½œIDå’Œæ•°å€¼å‚æ•°
                - 0: stop
                - 1: move forward
                - 2: turn left
                - 3: turn right
        """
        if "stop" in output:
            return 0, None
        elif "forward" in output:
            match = re.search(r'-?\d+', output)
            if match is None:
                return None, None
            return 1, float(match.group())
        elif "left" in output:
            match = re.search(r'-?\d+', output)
            if match is None:
                return None, None
            return 2, float(match.group())
        elif "right" in output:
            match = re.search(r'-?\d+', output)
            if match is None:
                return None, None
            return 3, float(match.group())

        return None, None


    def addtext(self, image, instuction, navigation):
        """
        åœ¨å›¾åƒä¸Šæ·»åŠ æŒ‡ä»¤å’Œå¯¼èˆªå†³ç­–æ–‡æœ¬ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        
        Args:
            image: åŸå§‹å›¾åƒ
            instuction: ä»»åŠ¡æŒ‡ä»¤æ–‡æœ¬
            navigation: å¯¼èˆªå†³ç­–æ–‡æœ¬
            
        Returns:
            æ·»åŠ æ–‡æœ¬åçš„å›¾åƒ
        """
        h, w = image.shape[:2]
        new_height = h + 150
        new_image = np.zeros((new_height, w, 3), np.uint8)
        new_image.fill(255)  
        new_image[:h, :w] = image

        font = cv2.FONT_HERSHEY_SIMPLEX
        textsize = cv2.getTextSize(instuction, font, 0.5, 2)[0]
        textY = h + (50 + textsize[1]) // 2
        y_line = textY + 0 * textsize[1]

        # è‡ªåŠ¨æ¢è¡Œå¤„ç†
        words = instuction.split(' ')
        x = 10
        line = ""

        for word in words:
            test_line = line + ' ' + word if line else word
            test_line_size, _ = cv2.getTextSize(test_line, font, 0.5, 2)

            if test_line_size[0] > image.shape[1] - x:
                cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)
                line = word
                y_line += textsize[1] + 5
            else:
                line = test_line

        if line:
            cv2.putText(new_image, line, (x, y_line), font, 0.5, (0, 0, 0), 2)

        # æ·»åŠ å¯¼èˆªå†³ç­–
        y_line = y_line + 1 * textsize[1] + 10
        new_image = cv2.putText(new_image, navigation, (x, y_line), font, 0.5, (0, 0, 0), 2)

        return new_image



    def reset(self):
        """é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€ï¼Œç”¨äºå¼€å§‹æ–°çš„episode"""
        # ä¿å­˜ä¸Šä¸€ä¸ªepisodeçš„å¯è§†åŒ–è§†é¢‘
        if self.require_map:
            if len(self.topdown_map_list) != 0:
                output_video_path = os.path.join(self.result_path, "video", "{}.gif".format(self.episode_id))
                imageio.mimsave(output_video_path, self.topdown_map_list)

        # æ¸…ç©ºçŠ¶æ€
        self.history_rgb_tensor = None
        self.transformation_list = []
        self.rgb_list = []
        self.topdown_map_list = []
        self.count_id += 1
        self.pending_action_list = []


    def act(self, observations, info, episode_id, sub_instruction=None):
        """
        æ‰§è¡Œå•æ­¥åŠ¨ä½œå†³ç­–
        
        Args:
            observations: ç¯å¢ƒè§‚æµ‹å­—å…¸ï¼Œç”±Habitatç¯å¢ƒè¿”å›ï¼ŒåŒ…å«ï¼š
                - "rgb": å½“å‰è§†è§’çš„RGBå›¾åƒ (numpyæ•°ç»„)
                - "instruction": å¯¼èˆªæŒ‡ä»¤å­—å…¸ï¼ŒåŒ…å«ï¼š
                    - "text": è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–‡æœ¬ (str)
                      ä¾‹å¦‚ï¼š"Go to the kitchen and find the refrigerator"
                      æ¥æºï¼šVLN-CEæ•°æ®é›†ä¸­çš„episodeå®šä¹‰
                      æ¯ä¸ªepisodeåœ¨æ•°æ®é›†ä¸­é¢„å…ˆå®šä¹‰äº†instruction_text
            info: ç¯å¢ƒä¿¡æ¯ï¼ˆåŒ…å«distance_to_goalç­‰æŒ‡æ ‡ï¼‰
            episode_id: å½“å‰episode ID
            sub_instruction: å¯é€‰çš„å­æŒ‡ä»¤æ–‡æœ¬ï¼Œå¦‚æœæä¾›åˆ™æ›¿æ¢ observations["instruction"]["text"]
            
        Returns:
            åŠ¨ä½œå­—å…¸ {"action": action_id}
        """
        self.episode_id = episode_id
        rgb = observations["rgb"]
        self.rgb_list.append(rgb)
        
        # ç¡®å®šä½¿ç”¨çš„æŒ‡ä»¤æ–‡æœ¬ï¼ˆå­æŒ‡ä»¤ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹æŒ‡ä»¤ï¼‰
        instruction_text = sub_instruction if sub_instruction is not None else observations["instruction"]["text"]

        # ç”Ÿæˆå¯è§†åŒ–åœ°å›¾
        if self.require_map:
            top_down_map = maps.colorize_draw_agent_and_fit_to_height(info["top_down_map_vlnce"], rgb.shape[0])
            output_im = np.concatenate((rgb, top_down_map), axis=1)

        # ã€åŠ¨ä½œç¼“å­˜æœºåˆ¶ã€‘é¿å…æ¯æ­¥éƒ½è°ƒç”¨è€—æ—¶çš„VLMæ¨ç†
        # åŸå› ï¼šæ¨¡å‹è¾“å‡ºé«˜å±‚æŒ‡ä»¤ï¼ˆå¦‚"å‰è¿›75cm"ï¼‰ï¼Œéœ€è¦æ‹†åˆ†ä¸ºå¤šä¸ªåº•å±‚åŠ¨ä½œï¼ˆ3ä¸ª"å‰è¿›25cm"ï¼‰
        # å¦‚æœç¼“å­˜ä¸­è¿˜æœ‰å¾…æ‰§è¡ŒåŠ¨ä½œï¼Œç›´æ¥è¿”å›ï¼Œæ— éœ€é‡æ–°æ¨ç†
        if len(self.pending_action_list) != 0:
            temp_action = self.pending_action_list.pop(0)  # å¼¹å‡ºé˜Ÿåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œ
            
            if self.require_map:
                img = self.addtext(output_im, observations["instruction"]["text"], 
                                   "Pending action: {}".format(temp_action))
                self.topdown_map_list.append(img)
            
            return {"action": temp_action}  # ç›´æ¥è¿”å›ç¼“å­˜åŠ¨ä½œï¼Œè·³è¿‡æ¨¡å‹æ¨ç†

        # ã€æ¨¡å‹æ¨ç†ã€‘ç¼“å­˜ä¸ºç©ºæ—¶ï¼Œæ‰è°ƒç”¨è€—æ—¶çš„VLMç”Ÿæˆæ–°å†³ç­–
        # 1. æ„å»ºå®Œæ•´çš„æç¤ºè¯
        navigation_qs = self.promt_template.format(instruction_text)
        
        # 2. è°ƒç”¨VLMæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆè€—æ—¶æ“ä½œï¼‰
        navigation = self.predict_inference(navigation_qs)  # GPUæ¨ç†ï¼Œçº¦2-3ç§’
        
        # 3. å¯è§†åŒ–ï¼šåœ¨åœ°å›¾ä¸Šå åŠ æ–‡æœ¬ï¼ˆç”¨äºç”ŸæˆGIFè§†é¢‘ï¼‰
        if self.require_map:
            # å°†instructionå’Œæ¨¡å‹è¾“å‡ºçš„å†³ç­–éƒ½æ ‡æ³¨åœ¨å›¾åƒä¸Š
            # ä¾‹å¦‚å›¾åƒåº•éƒ¨æ˜¾ç¤ºï¼š
            #   "Walk to the kitchen..."  (ä»»åŠ¡æŒ‡ä»¤/å­æŒ‡ä»¤)
            #   "turn left 60 degrees"    (æ¨¡å‹å†³ç­–)
            img = self.addtext(output_im, instruction_text, navigation)
            self.topdown_map_list.append(img)  # æ·»åŠ åˆ°è§†é¢‘å¸§åˆ—è¡¨

        # è§£æåŠ¨ä½œ
        action_index, num = self.extract_result(navigation[:-1])

        # ã€é«˜å±‚åˆ°åº•å±‚çš„åŠ¨ä½œåˆ†è§£ã€‘å°†æ¨¡å‹è¾“å‡ºçš„è¿ç»­æŒ‡ä»¤æ‹†åˆ†ä¸ºç¦»æ•£çš„åŸå­åŠ¨ä½œ
        # ä¾‹ï¼šæ¨¡å‹è¾“å‡º "move forward 75 meters" 
        #     â†’ æ‹†åˆ†ä¸º [1, 1, 1] ä¸‰ä¸ªåº•å±‚å‰è¿›åŠ¨ä½œï¼ˆæ¯ä¸ª25cmï¼‰
        #     â†’ ä¾æ¬¡æ‰§è¡Œï¼Œæ¯æ¬¡æ‰§è¡Œåç¯å¢ƒä¼šæ›´æ–°è§‚æµ‹
        if action_index == 0:
            self.pending_action_list.append(0)  # åœæ­¢åŠ¨ä½œä¸éœ€è¦æ‹†åˆ†
        elif action_index == 1:
            # å‰è¿›ï¼šæ¯æ¬¡åº•å±‚åŠ¨ä½œå‰è¿›25cmï¼Œæœ€å¤šæ’é˜Ÿ3æ­¥
            # ä¾‹ï¼šnum=75 â†’ int(75/25)=3 â†’ [1,1,1]
            for _ in range(min(3, int(num / 25))):
                self.pending_action_list.append(1)
        elif action_index == 2:
            # å·¦è½¬ï¼šæ¯æ¬¡åº•å±‚åŠ¨ä½œè½¬30åº¦ï¼Œæœ€å¤šæ’é˜Ÿ3æ­¥
            # ä¾‹ï¼šnum=90 â†’ int(90/30)=3 â†’ [2,2,2]
            for _ in range(min(3, int(num / 30))):
                self.pending_action_list.append(2)
        elif action_index == 3:
            # å³è½¬ï¼šæ¯æ¬¡åº•å±‚åŠ¨ä½œè½¬30åº¦ï¼Œæœ€å¤šæ’é˜Ÿ3æ­¥
            for _ in range(min(3, int(num / 30))):
                self.pending_action_list.append(3)
        
        # å®¹é”™ï¼šå¦‚æœè§£æå¤±è´¥æˆ–æ— åŠ¨ä½œï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªæ¢ç´¢åŠ¨ä½œ
        if action_index is None or len(self.pending_action_list) == 0:
            self.pending_action_list.append(random.randint(1, 3))

        # è¿”å›é˜Ÿåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œå‰©ä½™åŠ¨ä½œç•™åœ¨ç¼“å­˜ä¸­
        return {"action": self.pending_action_list.pop(0)}

