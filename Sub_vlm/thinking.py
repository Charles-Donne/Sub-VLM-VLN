"""
LLMè§„åˆ’ä¸æ€è€ƒæ¨¡å—
è´Ÿè´£å­ä»»åŠ¡ç”Ÿæˆã€å®ŒæˆéªŒè¯å’Œå¯¼èˆªè§„åˆ’
"""
import json
import requests
import base64
from typing import Dict, List, Tuple, Optional
from Sub_vlm.llm_config import LLMConfig
from Sub_vlm.prompts import (
    get_initial_planning_prompt,
    get_verification_prompt,
    get_task_completion_prompt
)


class SubTask:
    """å­ä»»åŠ¡æ•°æ®ç»“æ„"""
    
    def __init__(self, description: str, planning_hints: str, completion_criteria: str):
        """
        Args:
            description: å­ä»»åŠ¡æè¿°
            planning_hints: è§„åˆ’æç¤ºï¼ˆè¾…åŠ©æ€è€ƒï¼‰
            completion_criteria: å®Œæˆåˆ¤åˆ«æ ‡å‡†
        """
        self.description = description
        self.planning_hints = planning_hints
        self.completion_criteria = completion_criteria
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "description": self.description,
            "planning_hints": self.planning_hints,
            "completion_criteria": self.completion_criteria
        }
    
    def __repr__(self):
        return f"SubTask(description='{self.description[:50]}...')"


class LLMPlanner:
    """LLMè§„åˆ’å™¨ - è´Ÿè´£å­ä»»åŠ¡ç”Ÿæˆå’ŒéªŒè¯"""
    
    def __init__(self, config_path="llm_config.yaml"):
        """
        åˆå§‹åŒ–è§„åˆ’å™¨
        
        Args:
            config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = LLMConfig(config_path)
        print(f"âœ“ LLMè§„åˆ’å™¨åˆå§‹åŒ–å®Œæˆ: {self.config}")
    
    def encode_image_base64(self, image_path: str) -> str:
        """
        å°†å›¾åƒç¼–ç ä¸ºbase64
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            base64ç¼–ç çš„å›¾åƒå­—ç¬¦ä¸²
        """
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')
    
    def _build_initial_planning_prompt(self, instruction: str, direction_names: List[str]) -> str:
        """
        æ„å»ºåˆå§‹è§„åˆ’promptï¼ˆä»»åŠ¡å¼€å§‹æ—¶ï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            direction_names: æ–¹å‘åç§°åˆ—è¡¨ï¼ˆå¯¹åº”8å¼ å›¾ç‰‡ï¼‰
            
        Returns:
            promptæ–‡æœ¬
        """
        return get_initial_planning_prompt(instruction, direction_names)
    
    def _build_verification_prompt(self, 
                                   instruction: str,
                                   subtask: SubTask,
                                   direction_names: List[str]) -> str:
        """
        æ„å»ºéªŒè¯promptï¼ˆæ£€æŸ¥å­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            subtask: å½“å‰å­ä»»åŠ¡
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            promptæ–‡æœ¬
        """
        return get_verification_prompt(
            instruction,
            subtask.description,
            subtask.completion_criteria,
            subtask.planning_hints,
            direction_names
        )
    
    def _build_task_completion_prompt(self,
                                     instruction: str,
                                     direction_names: List[str]) -> str:
        """
        æ„å»ºä»»åŠ¡å®Œæˆæ£€æŸ¥prompt
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            promptæ–‡æœ¬
        """
        return get_task_completion_prompt(instruction, direction_names)
    
    def _call_llm_api(self, 
                     prompt: str, 
                     image_paths: List[str]) -> Optional[Dict]:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: æ–‡æœ¬prompt
            image_paths: å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            APIå“åº”çš„JSONæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = [{"type": "text", "text": prompt}]
            
            # æ·»åŠ å›¾åƒ
            for img_path in image_paths:
                img_base64 = self.encode_image_base64(img_path)
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_base64}"
                    }
                })
            
            # æ„å»ºè¯·æ±‚
            payload = {
                "model": self.config.model,
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens
            }
            
            # å‘é€è¯·æ±‚
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨LLM API ({self.config.model})...")
            response = requests.post(
                f"{self.config.base_url}/chat/completions",
                headers=self.config.get_headers(),
                json=payload,
                timeout=self.config.timeout
            )
            
            response.raise_for_status()
            
            # è§£æå“åº”
            result = response.json()
            content_text = result['choices'][0]['message']['content']
            
            # å°è¯•è§£æJSON
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content_text = content_text.strip()
            if content_text.startswith("```json"):
                content_text = content_text[7:]
            if content_text.startswith("```"):
                content_text = content_text[3:]
            if content_text.endswith("```"):
                content_text = content_text[:-3]
            content_text = content_text.strip()
            
            parsed_json = json.loads(content_text)
            print("âœ“ LLMå“åº”è§£ææˆåŠŸ")
            
            return parsed_json
            
        except requests.exceptions.RequestException as e:
            print(f"âœ— APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"âœ— JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {content_text[:500]}...")
            return None
        except Exception as e:
            print(f"âœ— æœªçŸ¥é”™è¯¯: {e}")
            return None
    
    def generate_initial_subtask(self,
                                instruction: str,
                                observation_images: List[str],
                                direction_names: List[str]) -> Optional[SubTask]:
        """
        ç”Ÿæˆåˆå§‹å­ä»»åŠ¡ï¼ˆä»»åŠ¡å¼€å§‹æ—¶ï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            SubTaskå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        prompt = self._build_initial_planning_prompt(instruction, direction_names)
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return None
        
        try:
            subtask = SubTask(
                description=response['subtask_instruction'],
                planning_hints=response['planning_hints'],
                completion_criteria=response['completion_criteria']
            )
            
            print(f"\nğŸ“‹ ç”Ÿæˆçš„å­ä»»åŠ¡:")
            print(f"  å½“å‰ä½ç½®: {response.get('current_location', 'N/A')}")
            print(f"  æŒ‡ä»¤åºåˆ—: {response.get('instruction_sequence', 'N/A')}")
            print(f"  å­ä»»åŠ¡ç›®çš„åœ°: {response.get('subtask_destination', 'N/A')}")
            print(f"  å­ä»»åŠ¡æŒ‡ä»¤: {subtask.description}")
            print(f"  è§„åˆ’æç¤º: {subtask.planning_hints}")
            print(f"  å®Œæˆæ ‡å‡†: {subtask.completion_criteria}")
            if 'reasoning' in response:
                print(f"  æ¨ç†è¿‡ç¨‹: {response['reasoning']}")
            
            return subtask
            
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return None
    
    def verify_and_plan_next(self,
                            instruction: str,
                            current_subtask: SubTask,
                            observation_images: List[str],
                            direction_names: List[str]) -> Tuple[bool, Optional[SubTask], Optional[str]]:
        """
        éªŒè¯å½“å‰å­ä»»åŠ¡å¹¶è§„åˆ’ä¸‹ä¸€ä¸ª
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            current_subtask: å½“å‰å­ä»»åŠ¡
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            (is_completed, next_subtask, advice)
            - is_completed: å½“å‰å­ä»»åŠ¡æ˜¯å¦å®Œæˆ
            - next_subtask: ä¸‹ä¸€ä¸ªå­ä»»åŠ¡ï¼ˆå¦‚æœå½“å‰å·²å®Œæˆï¼‰
            - advice: ç»§ç»­å®Œæˆçš„å»ºè®®ï¼ˆå¦‚æœæœªå®Œæˆï¼‰
        """
        prompt = self._build_verification_prompt(
            instruction, current_subtask, direction_names
        )
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return False, None, "APIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•éªŒè¯"
        
        try:
            is_completed = response['is_completed']
            analysis = response['completion_analysis']
            
            print(f"\nğŸ” å­ä»»åŠ¡éªŒè¯ç»“æœ:")
            print(f"  å®ŒæˆçŠ¶æ€: {'âœ“ å·²å®Œæˆ' if is_completed else 'âœ— æœªå®Œæˆ'}")
            print(f"  åˆ†æ: {analysis}")
            
            if is_completed:
                next_data = response['next_subtask']
                next_subtask = SubTask(
                    description=next_data['subtask_description'],
                    planning_hints=next_data['planning_hints'],
                    completion_criteria=next_data['completion_criteria']
                )
                
                print(f"\nğŸ“‹ ä¸‹ä¸€ä¸ªå­ä»»åŠ¡:")
                print(f"  æè¿°: {next_subtask.description}")
                print(f"  æç¤º: {next_subtask.planning_hints}")
                print(f"  æ ‡å‡†: {next_subtask.completion_criteria}")
                
                return True, next_subtask, None
            else:
                advice = response.get('continuation_advice', 'ç»§ç»­æŒ‰è®¡åˆ’æ‰§è¡Œ')
                print(f"  å»ºè®®: {advice}")
                return False, None, advice
                
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return False, None, "å“åº”æ ¼å¼é”™è¯¯"
    
    def check_task_completion(self,
                             instruction: str,
                             observation_images: List[str],
                             direction_names: List[str]) -> Tuple[bool, float, str]:
        """
        æ£€æŸ¥æ•´ä¸ªä»»åŠ¡æ˜¯å¦å®Œæˆ
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            (is_completed, confidence, analysis)
        """
        prompt = self._build_task_completion_prompt(instruction, direction_names)
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return False, 0.0, "APIè°ƒç”¨å¤±è´¥"
        
        try:
            is_completed = response['task_completed']
            confidence = response['confidence']
            analysis = response['analysis']
            
            print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆæ£€æŸ¥:")
            print(f"  çŠ¶æ€: {'âœ“ å·²å®Œæˆ' if is_completed else 'âœ— æœªå®Œæˆ'}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.2%}")
            print(f"  åˆ†æ: {analysis}")
            
            if not is_completed and 'recommendation' in response:
                print(f"  å»ºè®®: {response['recommendation']}")
            
            return is_completed, confidence, analysis
            
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return False, 0.0, "å“åº”æ ¼å¼é”™è¯¯"
