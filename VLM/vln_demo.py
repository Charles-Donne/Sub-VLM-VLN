import torch
import os
from pathlib import Path
from transformers import AutoModelForImageTextToText, AutoProcessor
from PIL import Image

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/Qwen3-VL-8B-Instruct"

# å›ºå®šçš„å•å¼ è§‚å¯Ÿå›¾åƒè·¯å¾„
OBSERVATION_IMAGE = "/root/autodl-tmp/current_observation/view.jpg"

# å¯¼èˆªæŒ‡ä»¤ï¼ˆè‹±æ–‡ï¼‰
NAVIGATION_INSTRUCTION = "Go to the kitchen"

print("=" * 70)
print("ğŸ¤– VLN Navigator Demo - Single Shot Navigation")
print("=" * 70)

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
print("\n[1/2] Loading model...")
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype="auto",
)
processor = AutoProcessor.from_pretrained(MODEL_PATH)

# è®¾ç½®å›¾åƒåƒç´ é¢„ç®—
processor.image_processor.size = {
    "longest_edge": 1280*32*32,
    "shortest_edge": 256*32*32
}

print("âœ… Model loaded successfully!")

def predict_navigation_action(instruction, image_path):
    """
    é¢„æµ‹ä¸‹ä¸€æ­¥å¯¼èˆªåŠ¨ä½œï¼ˆä¸€æ¬¡æ€§ Demoï¼‰
    
    Args:
        instruction: str - è‹±æ–‡å¯¼èˆªæŒ‡ä»¤
        image_path: str - å½“å‰è§‚å¯Ÿå›¾åƒè·¯å¾„
    
    Returns:
        str - é¢„æµ‹çš„åŠ¨ä½œ (å®Œæ•´åŠ¨ä½œæè¿°ï¼Œå¦‚ "Move forward 25 centimeters")
    """
    
    print("\n" + "=" * 70)
    print("ğŸ§­ VLN Navigation Demo")
    print("=" * 70)
    print(f"ğŸ“ Instruction: {instruction}")
    print(f"ğŸ“· Image: {image_path}")
    print("-" * 70)
    
    # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ Error: Image not found: {image_path}")
        return None
    
    # æ„å»ºè¯¦ç»†çš„ Promptï¼ˆæ‰€æœ‰ç»†èŠ‚éƒ½åœ¨è¿™é‡Œï¼‰
    detailed_prompt = f"""You are a vision-language navigation agent. You need to navigate in an indoor environment by following natural language instructions.

**TASK:**
Navigate to the goal location described in the instruction by analyzing the current RGB observation image.

**NAVIGATION INSTRUCTION:**
"{instruction}"

**CURRENT OBSERVATION:**
The image above shows your current first-person view in the environment.

**AVAILABLE ACTIONS:**
You can ONLY choose ONE of the following four actions:
1. Move forward 25 centimeters - Move straight forward by 25 centimeters
2. Turn left 45 degrees - Rotate left by 45 degrees (counterclockwise)
3. Turn right 45 degrees - Rotate right by 45 degrees (clockwise)
4. Move backward 25 centimeters - Move straight backward by 25 centimeters

**DECISION CRITERIA:**
- Analyze the visual scene carefully: identify rooms, objects, doorways, corridors, and spatial layout
- Understand the navigation instruction: determine the target location and required path
- Consider your current orientation and position relative to potential goals
- Choose the action that makes the most progress toward the goal
- Prioritize forward movement when the path is clear and aligned with the goal
- Use turning actions to adjust orientation when needed
- Use backward movement only when blocked and need to retreat

**OUTPUT REQUIREMENTS:**
- Output EXACTLY ONE complete action description
- Must be one of: "Move forward 25 centimeters", "Turn left 45 degrees", "Turn right 45 degrees", "Move backward 25 centimeters"
- Do NOT add any explanation, reasoning, or additional text
- Do NOT use phrases like "I think" or "The action is"
- Output format: Just the action description with exact measurements, nothing else

**EXAMPLES:**
Instruction: "Go to the kitchen"
Image: [shows a corridor with kitchen visible ahead]
Correct output: Move forward 25 centimeters

Instruction: "Turn to face the door"
Image: [shows door on the left side]
Correct output: Turn left 45 degrees

**YOUR TURN:**
Based on the instruction "{instruction}" and the current observation image above, what is the next action?

Output (one word only):"""
    
    try:
        # åŠ è½½å›¾åƒ
        print("ğŸ“¥ Loading image...")
        img = Image.open(image_path)
        if img.mode != 'RGB':
            img = img.convert('RGB')
        print(f"âœ… Image loaded: {img.size}, mode={img.mode}")
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆå›¾åƒ + è¯¦ç»† Promptï¼‰
        messages = [{
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": f"file://{os.path.abspath(image_path)}"
                },
                {
                    "type": "text",
                    "text": detailed_prompt
                }
            ]
        }]
        
        # å¤„ç†è¾“å…¥
        print("âš™ï¸  Processing input...")
        text = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        if isinstance(text, list):
            text = text[0] if text else ""
        
        inputs = processor(
            text=text,
            images=[img],
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        
        print("ğŸ¤” Predicting next action...")
        
        # ç”Ÿæˆå›å¤ï¼ˆä½æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡ºï¼‰
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=20,   # åªéœ€è¦ä¸€ä¸ªè¯
            do_sample=False,     # è´ªå¿ƒè§£ç 
            temperature=0.1,
        )
        
        # è§£ç è¾“å‡º
        generated_ids_trimmed = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # æå–åŠ¨ä½œ
        print(f"\nğŸ“¤ Raw output: '{output_text}'")
        action = parse_action(output_text)
        
        return action
        
    except Exception as e:
        print(f"âŒ Error during prediction: {e}")
        import traceback
        traceback.print_exc()
        return None

def parse_action(output_text):
    """ä¸¥æ ¼è§£æåŠ¨ä½œè¾“å‡ºï¼ˆç›´æ¥è¿”å›å®Œæ•´åŠ¨ä½œæè¿°ï¼‰"""
    # æ¸…ç†è¾“å‡º
    output_text = output_text.strip().lower()
    
    # ç›´æ¥åŒ¹é…å®Œæ•´åŠ¨ä½œæè¿°
    if "move forward 25 centimeters" in output_text or "move forward 25cm" in output_text:
        return "Move forward 25 centimeters"
    elif "turn left 45 degrees" in output_text:
        return "Turn left 45 degrees"
    elif "turn right 45 degrees" in output_text:
        return "Turn right 45 degrees"
    elif "move backward 25 centimeters" in output_text or "move backward 25cm" in output_text:
        return "Move backward 25 centimeters"
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœæ¨¡å‹è¾“å‡ºäº†ç®€åŒ–ç‰ˆæœ¬ï¼‰
    elif "forward" in output_text and "25" in output_text:
        return "Move forward 25 centimeters"
    elif "left" in output_text and "45" in output_text:
        return "Turn left 45 degrees"
    elif "right" in output_text and "45" in output_text:
        return "Turn right 45 degrees"
    elif "backward" in output_text and "25" in output_text:
        return "Move backward 25 centimeters"
    else:
        print(f"âš ï¸  Warning: Cannot parse valid action from: '{output_text}'")
        print("   Defaulting to forward movement")
        return "Move forward 25 centimeters"  # é»˜è®¤å‘å‰

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("\n[2/2] Running navigation demo...")
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    if not os.path.exists(OBSERVATION_IMAGE):
        print(f"\nâŒ Error: Observation image not found!")
        print(f"   Expected: {OBSERVATION_IMAGE}")
        print(f"   Please place your RGB observation image at this path.")
        exit(1)
    
    # æ‰§è¡Œå¯¼èˆªé¢„æµ‹
    action = predict_navigation_action(NAVIGATION_INSTRUCTION, OBSERVATION_IMAGE)
    
    # è¾“å‡ºç»“æœ
    if action:
        print("\n" + "=" * 70)
        print("âœ… NAVIGATION RESULT")
        print("=" * 70)
        print(f"ğŸ“ Instruction: {NAVIGATION_INSTRUCTION}")
        print(f"ğŸ¯ Predicted Action: {action}")
        print()
        print("=" * 70)
    else:
        print("\nâŒ Failed to predict action")
