import torch
from transformers import AutoModelForImageTextToText, AutoProcessor

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/Qwen3-VL-8B-Instruct"

# å†å²ç®¡ç†é…ç½®
MAX_HISTORY_TURNS = 10  # æœ€å¤šä¿ç•™æœ€è¿‘10è½®å¯¹è¯ï¼ˆ20æ¡æ¶ˆæ¯ï¼‰
MAX_TOKENS = 4096       # æœ€å¤§tokenæ•°é™åˆ¶

# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype="auto",
)
processor = AutoProcessor.from_pretrained(MODEL_PATH)
print("æ¨¡å‹åŠ è½½å®Œæˆ!")
print("=" * 60)
print("æ¬¢è¿ä½¿ç”¨ Qwen3-VL å¯¹è¯ç³»ç»Ÿ!")
print("è¾“å…¥ 'exit'ã€'quit' æˆ– 'q' é€€å‡ºå¯¹è¯")
print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
print("è¾“å…¥ 'status' æŸ¥çœ‹å½“å‰çŠ¶æ€")
print(f"ğŸ’¡ è‡ªåŠ¨ä¿ç•™æœ€è¿‘ {MAX_HISTORY_TURNS} è½®å¯¹è¯")
print("=" * 60)

# å¯¹è¯å†å²
conversation_history = []

def manage_history():
    """ç®¡ç†å¯¹è¯å†å²ï¼Œå®ç°é—å¿˜æœºåˆ¶"""
    global conversation_history
    
    # ç­–ç•¥1: æŒ‰è½®æ•°æˆªæ–­ï¼ˆç®€å•æœ‰æ•ˆï¼‰
    # æ¯è½®å¯¹è¯ = 1ä¸ªç”¨æˆ·æ¶ˆæ¯ + 1ä¸ªåŠ©æ‰‹å›å¤ = 2æ¡æ¶ˆæ¯
    if len(conversation_history) > MAX_HISTORY_TURNS * 2:
        # ä¿ç•™æœ€è¿‘çš„Nè½®å¯¹è¯
        conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]
        print(f"ğŸ’¡ æç¤º: å¯¹è¯å†å²å·²è¶…è¿‡{MAX_HISTORY_TURNS}è½®ï¼Œè‡ªåŠ¨æ¸…ç†äº†æ—©æœŸå¯¹è¯\n")
    
    # ç­–ç•¥2: æŒ‰tokenæ•°æˆªæ–­ï¼ˆæ›´ç²¾ç¡®ï¼‰
    # è®¡ç®—å½“å‰å†å²çš„å¤§è‡´tokenæ•°
    total_text = ""
    for msg in conversation_history:
        for content in msg["content"]:
            if content["type"] == "text":
                total_text += content["text"]
    
    # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡ 1å­—â‰ˆ1.5tokenï¼Œè‹±æ–‡ 1è¯â‰ˆ1.3token
    estimated_tokens = len(total_text) * 1.5
    
    # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œä»å¤´éƒ¨å¼€å§‹åˆ é™¤ï¼ˆä¿ç•™æœ€è¿‘çš„ï¼‰
    while estimated_tokens > MAX_TOKENS and len(conversation_history) > 2:
        # åˆ é™¤æœ€æ—©çš„ä¸€è½®å¯¹è¯ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰
        removed = conversation_history[:2]
        conversation_history = conversation_history[2:]
        
        # é‡æ–°è®¡ç®—
        removed_text = ""
        for msg in removed:
            for content in msg["content"]:
                if content["type"] == "text":
                    removed_text += content["text"]
        estimated_tokens -= len(removed_text) * 1.5

def generate_response(user_input):
    """ç”Ÿæˆæ¨¡å‹å›å¤"""
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    conversation_history.append({
        "role": "user",
        "content": [
            {"type": "text", "text": user_input}
        ]
    })
    
    # ğŸ”¥ æ‰§è¡Œé—å¿˜æœºåˆ¶
    manage_history()
    
    # å¤„ç†è¾“å…¥
    text = processor.apply_chat_template(
        conversation_history, 
        tokenize=False, 
        add_generation_prompt=True
    )
    inputs = processor(
        text=[text],
        return_tensors="pt",
    ).to(model.device)
    
    # ç”Ÿæˆå›å¤
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.8,
        repetition_penalty=1.05
    )
    
    # è§£ç è¾“å‡º
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    conversation_history.append({
        "role": "assistant",
        "content": [
            {"type": "text", "text": output_text}
        ]
    })
    
    return output_text

# ä¸»å¯¹è¯å¾ªç¯
print("\nğŸ¤– åŠ©æ‰‹: ä½ å¥½ï¼æˆ‘æ˜¯ Qwenï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n")

while True:
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        
        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if user_input.lower() in ['exit', 'quit', 'q']:
            print("\nå†è§ï¼ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼ğŸ‘‹")
            break
        
        # æ£€æŸ¥æ¸…ç©ºå†å²å‘½ä»¤
        if user_input.lower() == 'clear':
            conversation_history = []
            print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©ºï¼\n")
            continue
        
        # æ˜¾ç¤ºå½“å‰å†å²çŠ¶æ€
        if user_input.lower() == 'status':
            turns = len(conversation_history) // 2
            print(f"\nğŸ“Š å½“å‰çŠ¶æ€:")
            print(f"   - å¯¹è¯è½®æ•°: {turns}")
            print(f"   - å†å²æ¶ˆæ¯æ•°: {len(conversation_history)}")
            print(f"   - æœ€å¤§ä¿ç•™è½®æ•°: {MAX_HISTORY_TURNS}\n")
            continue
        
        # è·³è¿‡ç©ºè¾“å…¥
        if not user_input:
            continue
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›å¤
        print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        response = generate_response(user_input)
        print(response + "\n")
        
    except KeyboardInterrupt:
        print("\n\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
        break
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}\n")
        # å‘ç”Ÿé”™è¯¯æ—¶ç§»é™¤æœ€åæ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
        if conversation_history and conversation_history[-1]["role"] == "user":
            conversation_history.pop()
