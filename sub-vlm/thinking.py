"""
LLMè§„åˆ’ä¸æ€è€ƒæ¨¡å—
è´Ÿè´£å­ä»»åŠ¡ç”Ÿæˆã€å®ŒæˆéªŒè¯å’Œå¯¼èˆªè§„åˆ’
"""
import json
import requests
import base64
from typing import Dict, List, Tuple, Optional
from llm_config import LLMConfig


class SubTask:
    """å­ä»»åŠ¡æ•°æ®ç»“æ„"""
    
    def __init__(self, description: str, planning_hints: str, completion_criteria: str):
        """
        Args:
            description: å­ä»»åŠ¡æè¿°
            planning_hints: è§„åˆ’æç¤ºï¼ˆè¾…åŠ©æ€è€ƒï¼‰
            completion_criteria: å®Œæˆåˆ¤åˆ«æ ‡å‡†
        """
        self.description = description
        self.planning_hints = planning_hints
        self.completion_criteria = completion_criteria
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "description": self.description,
            "planning_hints": self.planning_hints,
            "completion_criteria": self.completion_criteria
        }
    
    def __repr__(self):
        return f"SubTask(description='{self.description[:50]}...')"


class LLMPlanner:
    """LLMè§„åˆ’å™¨ - è´Ÿè´£å­ä»»åŠ¡ç”Ÿæˆå’ŒéªŒè¯"""
    
    def __init__(self, config_path="llm_config.yaml"):
        """
        åˆå§‹åŒ–è§„åˆ’å™¨
        
        Args:
            config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = LLMConfig(config_path)
        print(f"âœ“ LLMè§„åˆ’å™¨åˆå§‹åŒ–å®Œæˆ: {self.config}")
    
    def encode_image_base64(self, image_path: str) -> str:
        """
        å°†å›¾åƒç¼–ç ä¸ºbase64
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            base64ç¼–ç çš„å›¾åƒå­—ç¬¦ä¸²
        """
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')
    
    def _build_initial_planning_prompt(self, instruction: str, direction_names: List[str]) -> str:
        """
        æ„å»ºåˆå§‹è§„åˆ’promptï¼ˆä»»åŠ¡å¼€å§‹æ—¶ï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            direction_names: æ–¹å‘åç§°åˆ—è¡¨ï¼ˆå¯¹åº”8å¼ å›¾ç‰‡ï¼‰
            
        Returns:
            promptæ–‡æœ¬
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡çš„è§„åˆ’åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©æ™ºèƒ½ä½“å®Œæˆå®¤å†…å¯¼èˆªä»»åŠ¡ã€‚

# å½“å‰ä»»åŠ¡
å®Œæ•´å¯¼èˆªæŒ‡ä»¤: {instruction}

# è§‚å¯Ÿä¿¡æ¯
æˆ‘ä¼šæä¾›8ä¸ªæ–¹å‘çš„è§‚å¯Ÿå›¾åƒï¼ˆä»å‰æ–¹å¼€å§‹é¡ºæ—¶é’ˆï¼‰ï¼š
{', '.join(direction_names)}

# ä½ çš„ä»»åŠ¡
åŸºäºå®Œæ•´æŒ‡ä»¤å’Œå½“å‰è§‚å¯Ÿï¼Œç”Ÿæˆç¬¬ä¸€ä¸ªå­ä»»åŠ¡ã€‚å­ä»»åŠ¡åº”è¯¥æ˜¯å¯å®Œæˆçš„å°æ­¥éª¤ã€‚

# è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{{
    "subtask_description": "å…·ä½“çš„å­ä»»åŠ¡æè¿°ï¼Œä¾‹å¦‚ï¼šå‘å‰èµ°åˆ°èµ°å»Šå°½å¤´çš„é—¨å£",
    "planning_hints": "å®Œæˆå­ä»»åŠ¡çš„è§„åˆ’æç¤ºï¼Œä¾‹å¦‚ï¼šä¿æŒç›´è¡Œï¼Œæ³¨æ„è§‚å¯Ÿå‰æ–¹æ˜¯å¦æœ‰é—¨æˆ–å¢™å£ã€‚é¢„è®¡éœ€è¦5-8æ­¥å‰è¿›",
    "completion_criteria": "åˆ¤æ–­å­ä»»åŠ¡å®Œæˆçš„æ ‡å‡†ï¼Œä¾‹å¦‚ï¼šå‰æ–¹å¯è§é—¨æ¡†æˆ–å¢™å£è·ç¦»å°äº2ç±³",
    "reasoning": "ä½ çš„æ¨ç†è¿‡ç¨‹ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå­ä»»åŠ¡"
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å­ä»»åŠ¡è¦å…·ä½“ã€å¯æ‰§è¡Œ
2. è§„åˆ’æç¤ºè¦åŒ…å«æ–¹å‘ã€è·ç¦»ã€æ ‡å¿—ç‰©ç­‰å…³é”®ä¿¡æ¯
3. å®Œæˆæ ‡å‡†è¦æ˜ç¡®ã€å¯è§‚å¯Ÿ
4. è€ƒè™‘æ™ºèƒ½ä½“çš„å®é™…èƒ½åŠ›ï¼šåªèƒ½å‰è¿›ã€å·¦è½¬ã€å³è½¬ã€åœæ­¢
"""
        return prompt
    
    def _build_verification_prompt(self, 
                                   instruction: str,
                                   subtask: SubTask,
                                   direction_names: List[str]) -> str:
        """
        æ„å»ºéªŒè¯promptï¼ˆæ£€æŸ¥å­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            subtask: å½“å‰å­ä»»åŠ¡
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            promptæ–‡æœ¬
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡çš„éªŒè¯åŠ©æ‰‹ã€‚ä½ éœ€è¦åˆ¤æ–­å½“å‰å­ä»»åŠ¡æ˜¯å¦å·²å®Œæˆã€‚

# ä»»åŠ¡èƒŒæ™¯
å®Œæ•´å¯¼èˆªæŒ‡ä»¤: {instruction}

# å½“å‰å­ä»»åŠ¡
- æè¿°: {subtask.description}
- å®Œæˆæ ‡å‡†: {subtask.completion_criteria}
- è§„åˆ’æç¤º: {subtask.planning_hints}

# å½“å‰è§‚å¯Ÿ
æˆ‘ä¼šæä¾›8ä¸ªæ–¹å‘çš„è§‚å¯Ÿå›¾åƒï¼ˆä»å‰æ–¹å¼€å§‹é¡ºæ—¶é’ˆï¼‰ï¼š
{', '.join(direction_names)}

# ä½ çš„ä»»åŠ¡
1. åˆ¤æ–­å½“å‰å­ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
2. å¦‚æœå·²å®Œæˆï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå­ä»»åŠ¡
3. å¦‚æœæœªå®Œæˆï¼Œç»™å‡ºç»§ç»­å®Œæˆçš„å»ºè®®

# è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{{
    "is_completed": true/false,
    "completion_analysis": "åˆ†æå­ä»»åŠ¡å®Œæˆæƒ…å†µçš„è¯¦ç»†è¯´æ˜",
    "next_subtask": {{
        "subtask_description": "ä¸‹ä¸€ä¸ªå­ä»»åŠ¡æè¿°ï¼ˆå¦‚æœå½“å‰å·²å®Œæˆï¼‰",
        "planning_hints": "è§„åˆ’æç¤º",
        "completion_criteria": "å®Œæˆæ ‡å‡†"
    }},
    "continuation_advice": "å¦‚æœæœªå®Œæˆï¼Œç»™å‡ºç»§ç»­å®Œæˆçš„å»ºè®®ï¼ˆå¦‚æœå·²å®Œæˆåˆ™ä¸ºnullï¼‰"
}}

æ³¨æ„äº‹é¡¹ï¼š
1. ä»”ç»†å¯¹æ¯”è§‚å¯Ÿå›¾åƒå’Œå®Œæˆæ ‡å‡†
2. å¦‚æœç”Ÿæˆä¸‹ä¸€ä¸ªå­ä»»åŠ¡ï¼Œè¦ç¡®ä¿å®ƒä¸æ€»ä½“æŒ‡ä»¤ä¸€è‡´
3. å»ºè®®è¦å…·ä½“ã€å¯æ“ä½œ
"""
        return prompt
    
    def _build_task_completion_prompt(self,
                                     instruction: str,
                                     direction_names: List[str]) -> str:
        """
        æ„å»ºä»»åŠ¡å®Œæˆæ£€æŸ¥prompt
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            promptæ–‡æœ¬
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡çš„éªŒè¯åŠ©æ‰‹ã€‚ä½ éœ€è¦åˆ¤æ–­æ•´ä¸ªå¯¼èˆªä»»åŠ¡æ˜¯å¦å·²å®Œæˆã€‚

# å®Œæ•´å¯¼èˆªæŒ‡ä»¤
{instruction}

# å½“å‰è§‚å¯Ÿ
æˆ‘ä¼šæä¾›8ä¸ªæ–¹å‘çš„è§‚å¯Ÿå›¾åƒï¼ˆä»å‰æ–¹å¼€å§‹é¡ºæ—¶é’ˆï¼‰ï¼š
{', '.join(direction_names)}

# ä½ çš„ä»»åŠ¡
åˆ¤æ–­æ™ºèƒ½ä½“æ˜¯å¦å·²ç»åˆ°è¾¾æŒ‡ä»¤æè¿°çš„ç›®æ ‡ä½ç½®ã€‚

# è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{{
    "task_completed": true/false,
    "confidence": 0.0-1.0,
    "analysis": "è¯¦ç»†åˆ†æä¸ºä»€ä¹ˆè®¤ä¸ºä»»åŠ¡å·²å®Œæˆæˆ–æœªå®Œæˆ",
    "recommendation": "å¦‚æœæœªå®Œæˆï¼Œå»ºè®®ä¸‹ä¸€æ­¥åšä»€ä¹ˆ"
}}

æ³¨æ„äº‹é¡¹ï¼š
1. ä»”ç»†å¯¹æ¯”å½“å‰è§‚å¯Ÿå’ŒæŒ‡ä»¤ä¸­æè¿°çš„ç›®æ ‡ä½ç½®
2. ç»™å‡ºä½ çš„ç½®ä¿¡åº¦è¯„åˆ†
3. åˆ†æè¦åŸºäºè§†è§‰è¯æ®
"""
        return prompt
    
    def _call_llm_api(self, 
                     prompt: str, 
                     image_paths: List[str]) -> Optional[Dict]:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: æ–‡æœ¬prompt
            image_paths: å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            APIå“åº”çš„JSONæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = [{"type": "text", "text": prompt}]
            
            # æ·»åŠ å›¾åƒ
            for img_path in image_paths:
                img_base64 = self.encode_image_base64(img_path)
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_base64}"
                    }
                })
            
            # æ„å»ºè¯·æ±‚
            payload = {
                "model": self.config.model,
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens
            }
            
            # å‘é€è¯·æ±‚
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨LLM API ({self.config.model})...")
            response = requests.post(
                f"{self.config.base_url}/chat/completions",
                headers=self.config.get_headers(),
                json=payload,
                timeout=self.config.timeout
            )
            
            response.raise_for_status()
            
            # è§£æå“åº”
            result = response.json()
            content_text = result['choices'][0]['message']['content']
            
            # å°è¯•è§£æJSON
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content_text = content_text.strip()
            if content_text.startswith("```json"):
                content_text = content_text[7:]
            if content_text.startswith("```"):
                content_text = content_text[3:]
            if content_text.endswith("```"):
                content_text = content_text[:-3]
            content_text = content_text.strip()
            
            parsed_json = json.loads(content_text)
            print("âœ“ LLMå“åº”è§£ææˆåŠŸ")
            
            return parsed_json
            
        except requests.exceptions.RequestException as e:
            print(f"âœ— APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"âœ— JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {content_text[:500]}...")
            return None
        except Exception as e:
            print(f"âœ— æœªçŸ¥é”™è¯¯: {e}")
            return None
    
    def generate_initial_subtask(self,
                                instruction: str,
                                observation_images: List[str],
                                direction_names: List[str]) -> Optional[SubTask]:
        """
        ç”Ÿæˆåˆå§‹å­ä»»åŠ¡ï¼ˆä»»åŠ¡å¼€å§‹æ—¶ï¼‰
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            SubTaskå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        prompt = self._build_initial_planning_prompt(instruction, direction_names)
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return None
        
        try:
            subtask = SubTask(
                description=response['subtask_description'],
                planning_hints=response['planning_hints'],
                completion_criteria=response['completion_criteria']
            )
            
            print(f"\nğŸ“‹ ç”Ÿæˆçš„å­ä»»åŠ¡:")
            print(f"  æè¿°: {subtask.description}")
            print(f"  æç¤º: {subtask.planning_hints}")
            print(f"  æ ‡å‡†: {subtask.completion_criteria}")
            if 'reasoning' in response:
                print(f"  æ¨ç†: {response['reasoning']}")
            
            return subtask
            
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return None
    
    def verify_and_plan_next(self,
                            instruction: str,
                            current_subtask: SubTask,
                            observation_images: List[str],
                            direction_names: List[str]) -> Tuple[bool, Optional[SubTask], Optional[str]]:
        """
        éªŒè¯å½“å‰å­ä»»åŠ¡å¹¶è§„åˆ’ä¸‹ä¸€ä¸ª
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            current_subtask: å½“å‰å­ä»»åŠ¡
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            (is_completed, next_subtask, advice)
            - is_completed: å½“å‰å­ä»»åŠ¡æ˜¯å¦å®Œæˆ
            - next_subtask: ä¸‹ä¸€ä¸ªå­ä»»åŠ¡ï¼ˆå¦‚æœå½“å‰å·²å®Œæˆï¼‰
            - advice: ç»§ç»­å®Œæˆçš„å»ºè®®ï¼ˆå¦‚æœæœªå®Œæˆï¼‰
        """
        prompt = self._build_verification_prompt(
            instruction, current_subtask, direction_names
        )
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return False, None, "APIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•éªŒè¯"
        
        try:
            is_completed = response['is_completed']
            analysis = response['completion_analysis']
            
            print(f"\nğŸ” å­ä»»åŠ¡éªŒè¯ç»“æœ:")
            print(f"  å®ŒæˆçŠ¶æ€: {'âœ“ å·²å®Œæˆ' if is_completed else 'âœ— æœªå®Œæˆ'}")
            print(f"  åˆ†æ: {analysis}")
            
            if is_completed:
                next_data = response['next_subtask']
                next_subtask = SubTask(
                    description=next_data['subtask_description'],
                    planning_hints=next_data['planning_hints'],
                    completion_criteria=next_data['completion_criteria']
                )
                
                print(f"\nğŸ“‹ ä¸‹ä¸€ä¸ªå­ä»»åŠ¡:")
                print(f"  æè¿°: {next_subtask.description}")
                print(f"  æç¤º: {next_subtask.planning_hints}")
                print(f"  æ ‡å‡†: {next_subtask.completion_criteria}")
                
                return True, next_subtask, None
            else:
                advice = response.get('continuation_advice', 'ç»§ç»­æŒ‰è®¡åˆ’æ‰§è¡Œ')
                print(f"  å»ºè®®: {advice}")
                return False, None, advice
                
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return False, None, "å“åº”æ ¼å¼é”™è¯¯"
    
    def check_task_completion(self,
                             instruction: str,
                             observation_images: List[str],
                             direction_names: List[str]) -> Tuple[bool, float, str]:
        """
        æ£€æŸ¥æ•´ä¸ªä»»åŠ¡æ˜¯å¦å®Œæˆ
        
        Args:
            instruction: å®Œæ•´å¯¼èˆªæŒ‡ä»¤
            observation_images: 8ä¸ªæ–¹å‘çš„å›¾åƒè·¯å¾„åˆ—è¡¨
            direction_names: æ–¹å‘åç§°åˆ—è¡¨
            
        Returns:
            (is_completed, confidence, analysis)
        """
        prompt = self._build_task_completion_prompt(instruction, direction_names)
        
        response = self._call_llm_api(prompt, observation_images)
        
        if response is None:
            return False, 0.0, "APIè°ƒç”¨å¤±è´¥"
        
        try:
            is_completed = response['task_completed']
            confidence = response['confidence']
            analysis = response['analysis']
            
            print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆæ£€æŸ¥:")
            print(f"  çŠ¶æ€: {'âœ“ å·²å®Œæˆ' if is_completed else 'âœ— æœªå®Œæˆ'}")
            print(f"  ç½®ä¿¡åº¦: {confidence:.2%}")
            print(f"  åˆ†æ: {analysis}")
            
            if not is_completed and 'recommendation' in response:
                print(f"  å»ºè®®: {response['recommendation']}")
            
            return is_completed, confidence, analysis
            
        except KeyError as e:
            print(f"âœ— å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
            return False, 0.0, "å“åº”æ ¼å¼é”™è¯¯"
