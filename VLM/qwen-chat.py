import torch
from transformers import AutoModelForImageTextToText, AutoProcessor

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/Qwen3-VL-8B-Instruct"

# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype="auto",
)
processor = AutoProcessor.from_pretrained(MODEL_PATH)
print("æ¨¡å‹åŠ è½½å®Œæˆ!")
print("=" * 60)
print("æ¬¢è¿ä½¿ç”¨ Qwen3-VL å¯¹è¯ç³»ç»Ÿ!")
print("è¾“å…¥ 'exit'ã€'quit' æˆ– 'q' é€€å‡ºå¯¹è¯")
print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
print("=" * 60)

# å¯¹è¯å†å²
conversation_history = []

def generate_response(user_input):
    """ç”Ÿæˆæ¨¡å‹å›å¤"""
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    conversation_history.append({
        "role": "user",
        "content": [
            {"type": "text", "text": user_input}
        ]
    })
    
    # å¤„ç†è¾“å…¥
    text = processor.apply_chat_template(
        conversation_history, 
        tokenize=False, 
        add_generation_prompt=True
    )
    inputs = processor(
        text=[text],
        return_tensors="pt",
    ).to(model.device)
    
    # ç”Ÿæˆå›å¤
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.8,
        repetition_penalty=1.05
    )
    
    # è§£ç è¾“å‡º
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    conversation_history.append({
        "role": "assistant",
        "content": [
            {"type": "text", "text": output_text}
        ]
    })
    
    return output_text

# ä¸»å¯¹è¯å¾ªç¯
print("\nğŸ¤– åŠ©æ‰‹: ä½ å¥½ï¼æˆ‘æ˜¯ Qwenï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n")

while True:
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        
        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if user_input.lower() in ['exit', 'quit', 'q']:
            print("\nå†è§ï¼ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼ğŸ‘‹")
            break
        
        # æ£€æŸ¥æ¸…ç©ºå†å²å‘½ä»¤
        if user_input.lower() == 'clear':
            conversation_history = []
            print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©ºï¼\n")
            continue
        
        # è·³è¿‡ç©ºè¾“å…¥
        if not user_input:
            continue
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›å¤
        print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        response = generate_response(user_input)
        print(response + "\n")
        
    except KeyboardInterrupt:
        print("\n\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
        break
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}\n")
        # å‘ç”Ÿé”™è¯¯æ—¶ç§»é™¤æœ€åæ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
        if conversation_history and conversation_history[-1]["role"] == "user":
            conversation_history.pop()
